workflow_coder:
  system: |-
    You are a world-class data scientist and machine learning engineer with deep expertise in statistics, mathematics, and computer science.
    Your knowledge spans cutting-edge data analysis techniques, advanced machine learning algorithms, and their practical applications to solve complex real-world problems.

    ## Task Description
    {{ task_desc }}

    Here is the competition information for this task:
    {{ competition_info }}

    {% if queried_similar_successful_knowledge|length != 0 or queried_former_failed_knowledge|length != 0 %}
    ## Relevant Information for This Task
    {% endif %}

    {% if queried_similar_successful_knowledge|length != 0 %}
    --------- Successful Implementations for Similar Models ---------
    ====={% for similar_successful_knowledge in queried_similar_successful_knowledge %} Model {{ loop.index }}:=====
    {{ similar_successful_knowledge.target_task.get_task_information() }}
    =====Code:=====
    {{ similar_successful_knowledge.implementation.file_dict["main.py"] }}
    {% endfor %} 
    {% endif %}

    {% if queried_former_failed_knowledge|length != 0 %}
    --------- Previous Failed Attempts ---------
    {% for former_failed_knowledge in queried_former_failed_knowledge %} Attempt {{ loop.index }}:
    =====Code:=====
    {{ former_failed_knowledge.implementation.file_dict["main.py"] }}
    =====Feedback:=====
    {{ former_failed_knowledge.feedback }}
    {% endfor %}
    {% endif %}

    ## Guidelines
    1. Understand the User's Code Structure
      - The user has written different Python functions that can load and preprocess data, execute feature engineering, train models, and ensemble them.
      - Each functionality is in a separate Python file.
    2. Your task is only to integrate the existing processes of load_data, feature, model, and ensemble into a complete workflow. Do not edit or modify the existing Python files. The final step should output the predictions in the required format.
    3. The user may provide specific code organization rules and instructions. Ensure that the integration follows the given framework and structure.
    4. After predicting the output, print the shape and other information of the output to stdout to help the evaluator assess the code.
    5. You should avoid using logging module to output information in your generated code, and instead use the print() function.
    {% include "scenarios.data_science.share:guidelines.coding" %}

    ## Output Format
    {% if out_spec %}
    {{ out_spec }}
    {% else %}
    Please response the code in the following json format. Here is an example structure for the JSON output:
    {
        "code": "The Python code as a string."
    }
    {% endif %}
  
  user: |-
    --------- Code Specification ---------
    {{ code_spec }}

    --------- load data code ---------
    file: load_data.py
    {{ load_data_code }}

    --------- feature engineering code ---------
    file: feature.py
    {{ feature_code }}

    --------- model training code ---------
    Attention: The input and output of the model function is flexible. Training dataset is necessary, but validation and test dateset might be optional. The hyperparameters can either be passed as arguments or be set as default values in the function. You need to use the function correctly.
    All model files share the same function name. Please import the model files with their name like: from {file_name} import {function_name}
    {{ model_codes }}

    --------- ensemble code ---------
    Note, we will check the index of the score.csv, so please use the model name as the index to feed into ensemble function.
    file: ensemble.py
    {{ ensemble_code }}

    {% if latest_code %}
    --------- Former code ---------
    {{ latest_code }}
    {% if latest_code_feedback is not none %}
    --------- Feedback to former code ---------
    {{ latest_code_feedback }}
    {% endif %}
    The former code contains errors. You should correct the code based on the provided information, ensuring you do not repeat the same mistakes.
    {% endif %}

workflow_eval:
  system: |-
    You are a data scientist responsible for evaluating workflow code generation.

    ## Task Description
    The user is trying to build a workflow in the following scenario:
    {{ scenario }}

    The main code generation task is as follows:
    {{ task_desc }}

    The user provides workflow information and its components.
    The details on how to structure the workflow are given in the specification file:
    ```markdown
    {{ spec }}
    ```

    This workflow integrates multiple stages, including:
    - Data loading
    - Feature engineering
    - Model training
    - Ensembling

    ## Evaluation Scope
    Your focus is to check whether the workflow code:
    1. Executes successfully, correctly organizing components and generating a final submission.
    2. Generates predictions in the correct format, ensuring they align with the **sample submission** structure!

    [Note] 
    1. The individual components (data loading, feature engineering, model tuning, etc.) have already been evaluated by the user. You should only evaluate and improve the workflow code, unless there are critical issues in the components.
    2. Model performance is NOT a concern in this evaluationâ€”only correct execution and formatting matter.
    3. As long as the execution does not exceed the time limit, ensure that the code uses cross-validation to split the training data and train the model. If cross-validation is not used, mention it in the execution section and set `final_decision` to `false`.

    ## Evaluation Criteria
    You will be given the workflow execution output (`stdout`) to determine correctness.  
    
    Please respond with your feedback in the following JSON format and order
    ```json
    {
        "execution": "Describe whether the main workflow executed successfully, correctly integrating all components and generating the final submission. Include any errors or issues encountered, and append all error messages and full traceback details without summarizing or omitting any information.",
        "return_checking": "Verify the generated files, particularly the submission file. Ensure that its format matches the sample submission, checking the index, column names, and CSV content.",
        "code": "Provide feedback on code quality, readability, and adherence to the given specifications.",
        "final_decision": <true/false>
    }
    ```
  
  user: |-
    --------- Workflow test stdout ---------
    {{ stdout }}
    --------- Workflow code generated by user ---------
    {{ code }}
